<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.1 Functional Decompositon | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="5.1 Functional Decompositon | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.1 Functional Decompositon | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2021-09-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="agnostic.html"/>
<link rel="next" href="pdp.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="preface-by-the-author.html"><a href="preface-by-the-author.html"><i class="fa fa-check"></i>Preface by the Author</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> Story Time</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#lightning-never-strikes-twice"><i class="fa fa-check"></i>Lightning Never Strikes Twice</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#trust-fall"><i class="fa fa-check"></i>Trust Fall</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#fermis-paperclips"><i class="fa fa-check"></i>Fermi’s Paperclips</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> Importance of Interpretability</a></li>
<li class="chapter" data-level="2.2" data-path="taxonomy-of-interpretability-methods.html"><a href="taxonomy-of-interpretability-methods.html"><i class="fa fa-check"></i><b>2.2</b> Taxonomy of Interpretability Methods</a></li>
<li class="chapter" data-level="2.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>2.3</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.3.1</b> Algorithm Transparency</a></li>
<li class="chapter" data-level="2.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>2.3.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="2.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>2.3.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="2.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>2.3.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="2.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-predictions"><i class="fa fa-check"></i><b>2.3.5</b> Local Interpretability for a Group of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="evaluation-of-interpretability.html"><a href="evaluation-of-interpretability.html"><i class="fa fa-check"></i><b>2.4</b> Evaluation of Interpretability</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> Properties of Explanations</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> Human-friendly Explanations</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>2.6.1</b> What Is an Explanation?</a></li>
<li class="chapter" data-level="2.6.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>2.6.2</b> What Is a Good Explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Bike Rentals (Regression)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> Interpretable Models</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>4.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>4.1.2</b> Example</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>4.1.3</b> Visual Interpretation</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#explain-individual-predictions"><i class="fa fa-check"></i><b>4.1.4</b> Explain Individual Predictions</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>4.1.5</b> Encoding of Categorical Features</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>4.1.6</b> Do Linear Models Create Good Explanations?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> Sparse Linear Models</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>4.1.8</b> Advantages</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>4.1.9</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#what-is-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>4.2.1</b> What is Wrong with Linear Regression for Classification?</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#theory"><i class="fa fa-check"></i><b>4.2.2</b> Theory</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>4.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#example-1"><i class="fa fa-check"></i><b>4.2.4</b> Example</a></li>
<li class="chapter" data-level="4.2.5" data-path="logistic.html"><a href="logistic.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>4.2.5</b> Advantages and Disadvantages</a></li>
<li class="chapter" data-level="4.2.6" data-path="logistic.html"><a href="logistic.html#software"><i class="fa fa-check"></i><b>4.2.6</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.3</b> GLM, GAM and more</a><ul>
<li class="chapter" data-level="4.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.3.1</b> Non-Gaussian Outcomes - GLMs</a></li>
<li class="chapter" data-level="4.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.3.2</b> Interactions</a></li>
<li class="chapter" data-level="4.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.3.3</b> Nonlinear Effects - GAMs</a></li>
<li class="chapter" data-level="4.3.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>4.3.4</b> Advantages</a></li>
<li class="chapter" data-level="4.3.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>4.3.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.3.6" data-path="extend-lm.html"><a href="extend-lm.html#software-1"><i class="fa fa-check"></i><b>4.3.6</b> Software</a></li>
<li class="chapter" data-level="4.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.3.7</b> Further Extensions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.4</b> Decision Tree</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>4.4.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree.html"><a href="tree.html#example-2"><i class="fa fa-check"></i><b>4.4.2</b> Example</a></li>
<li class="chapter" data-level="4.4.3" data-path="tree.html"><a href="tree.html#advantages-2"><i class="fa fa-check"></i><b>4.4.3</b> Advantages</a></li>
<li class="chapter" data-level="4.4.4" data-path="tree.html"><a href="tree.html#disadvantages-2"><i class="fa fa-check"></i><b>4.4.4</b> Disadvantages</a></li>
<li class="chapter" data-level="4.4.5" data-path="tree.html"><a href="tree.html#software-2"><i class="fa fa-check"></i><b>4.4.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.5</b> Decision Rules</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>4.5.1</b> Learn Rules from a Single Feature (OneR)</a></li>
<li class="chapter" data-level="4.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.5.4" data-path="rules.html"><a href="rules.html#advantages-3"><i class="fa fa-check"></i><b>4.5.4</b> Advantages</a></li>
<li class="chapter" data-level="4.5.5" data-path="rules.html"><a href="rules.html#disadvantages-3"><i class="fa fa-check"></i><b>4.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.5.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>4.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.6</b> RuleFit</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>4.6.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="4.6.2" data-path="rulefit.html"><a href="rulefit.html#theory-1"><i class="fa fa-check"></i><b>4.6.2</b> Theory</a></li>
<li class="chapter" data-level="4.6.3" data-path="rulefit.html"><a href="rulefit.html#advantages-4"><i class="fa fa-check"></i><b>4.6.3</b> Advantages</a></li>
<li class="chapter" data-level="4.6.4" data-path="rulefit.html"><a href="rulefit.html#disadvantages-4"><i class="fa fa-check"></i><b>4.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="4.6.5" data-path="rulefit.html"><a href="rulefit.html#software-and-alternative"><i class="fa fa-check"></i><b>4.6.5</b> Software and Alternative</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.7</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="4.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>4.7.1</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="4.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>4.7.2</b> K-Nearest Neighbors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>5.1</b> Functional Decompositon</a><ul>
<li class="chapter" data-level="5.1.1" data-path="decomposition.html"><a href="decomposition.html#deeper-into-the-mathematics"><i class="fa fa-check"></i><b>5.1.1</b> Deeper Into the Mathematics</a></li>
<li class="chapter" data-level="5.1.2" data-path="decomposition.html"><a href="decomposition.html#how-to-compute-the-components"><i class="fa fa-check"></i><b>5.1.2</b> How to Compute the Components</a></li>
<li class="chapter" data-level="5.1.3" data-path="decomposition.html"><a href="decomposition.html#statistical-regression-models"><i class="fa fa-check"></i><b>5.1.3</b> Statistical Regression Models</a></li>
<li class="chapter" data-level="5.1.4" data-path="decomposition.html"><a href="decomposition.html#functional-anova"><i class="fa fa-check"></i><b>5.1.4</b> Functional ANOVA</a></li>
<li class="chapter" data-level="5.1.5" data-path="decomposition.html"><a href="decomposition.html#generalized-functional-anova-for-dependent-features"><i class="fa fa-check"></i><b>5.1.5</b> Generalized functional ANOVA for dependent features</a></li>
<li class="chapter" data-level="5.1.6" data-path="decomposition.html"><a href="decomposition.html#viewing-other-methods-through-the-lens-of-decomposition"><i class="fa fa-check"></i><b>5.1.6</b> Viewing Other Methods Through the Lens of Decomposition</a></li>
<li class="chapter" data-level="5.1.7" data-path="decomposition.html"><a href="decomposition.html#advantages-5"><i class="fa fa-check"></i><b>5.1.7</b> Advantages</a></li>
<li class="chapter" data-level="5.1.8" data-path="decomposition.html"><a href="decomposition.html#disadvantages-5"><i class="fa fa-check"></i><b>5.1.8</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.2</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>5.2.1</b> Examples</a></li>
<li class="chapter" data-level="5.2.2" data-path="pdp.html"><a href="pdp.html#advantages-6"><i class="fa fa-check"></i><b>5.2.2</b> Advantages</a></li>
<li class="chapter" data-level="5.2.3" data-path="pdp.html"><a href="pdp.html#disadvantages-6"><i class="fa fa-check"></i><b>5.2.3</b> Disadvantages</a></li>
<li class="chapter" data-level="5.2.4" data-path="pdp.html"><a href="pdp.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>5.2.4</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.3</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ice.html"><a href="ice.html#examples-1"><i class="fa fa-check"></i><b>5.3.1</b> Examples</a></li>
<li class="chapter" data-level="5.3.2" data-path="ice.html"><a href="ice.html#advantages-7"><i class="fa fa-check"></i><b>5.3.2</b> Advantages</a></li>
<li class="chapter" data-level="5.3.3" data-path="ice.html"><a href="ice.html#disadvantages-7"><i class="fa fa-check"></i><b>5.3.3</b> Disadvantages</a></li>
<li class="chapter" data-level="5.3.4" data-path="ice.html"><a href="ice.html#software-and-alternatives-2"><i class="fa fa-check"></i><b>5.3.4</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.4</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.4.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>5.4.1</b> Motivation and Intuition</a></li>
<li class="chapter" data-level="5.4.2" data-path="ale.html"><a href="ale.html#theory-2"><i class="fa fa-check"></i><b>5.4.2</b> Theory</a></li>
<li class="chapter" data-level="5.4.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>5.4.3</b> Estimation</a></li>
<li class="chapter" data-level="5.4.4" data-path="ale.html"><a href="ale.html#examples-2"><i class="fa fa-check"></i><b>5.4.4</b> Examples</a></li>
<li class="chapter" data-level="5.4.5" data-path="ale.html"><a href="ale.html#advantages-8"><i class="fa fa-check"></i><b>5.4.5</b> Advantages</a></li>
<li class="chapter" data-level="5.4.6" data-path="ale.html"><a href="ale.html#disadvantages-8"><i class="fa fa-check"></i><b>5.4.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.4.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>5.4.7</b> Implementation and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.5</b> Feature Interaction</a><ul>
<li class="chapter" data-level="5.5.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>5.5.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="5.5.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>5.5.2</b> Theory: Friedman’s H-statistic</a></li>
<li class="chapter" data-level="5.5.3" data-path="interaction.html"><a href="interaction.html#examples-3"><i class="fa fa-check"></i><b>5.5.3</b> Examples</a></li>
<li class="chapter" data-level="5.5.4" data-path="interaction.html"><a href="interaction.html#advantages-9"><i class="fa fa-check"></i><b>5.5.4</b> Advantages</a></li>
<li class="chapter" data-level="5.5.5" data-path="interaction.html"><a href="interaction.html#disadvantages-9"><i class="fa fa-check"></i><b>5.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>5.5.6</b> Implementations</a></li>
<li class="chapter" data-level="5.5.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>5.5.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.6</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.6.1" data-path="feature-importance.html"><a href="feature-importance.html#theory-3"><i class="fa fa-check"></i><b>5.6.1</b> Theory</a></li>
<li class="chapter" data-level="5.6.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.6.2</b> Should I Compute Importance on Training or Test Data?</a></li>
<li class="chapter" data-level="5.6.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>5.6.3</b> Example and Interpretation</a></li>
<li class="chapter" data-level="5.6.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-10"><i class="fa fa-check"></i><b>5.6.4</b> Advantages</a></li>
<li class="chapter" data-level="5.6.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-10"><i class="fa fa-check"></i><b>5.6.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.6.6" data-path="feature-importance.html"><a href="feature-importance.html#software-and-alternatives-3"><i class="fa fa-check"></i><b>5.6.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.7</b> Global Surrogate</a><ul>
<li class="chapter" data-level="5.7.1" data-path="global.html"><a href="global.html#theory-4"><i class="fa fa-check"></i><b>5.7.1</b> Theory</a></li>
<li class="chapter" data-level="5.7.2" data-path="global.html"><a href="global.html#example-4"><i class="fa fa-check"></i><b>5.7.2</b> Example</a></li>
<li class="chapter" data-level="5.7.3" data-path="global.html"><a href="global.html#advantages-11"><i class="fa fa-check"></i><b>5.7.3</b> Advantages</a></li>
<li class="chapter" data-level="5.7.4" data-path="global.html"><a href="global.html#disadvantages-11"><i class="fa fa-check"></i><b>5.7.4</b> Disadvantages</a></li>
<li class="chapter" data-level="5.7.5" data-path="global.html"><a href="global.html#software-3"><i class="fa fa-check"></i><b>5.7.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.8</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>5.8.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="5.8.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>5.8.2</b> LIME for Text</a></li>
<li class="chapter" data-level="5.8.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.8.3</b> LIME for Images</a></li>
<li class="chapter" data-level="5.8.4" data-path="lime.html"><a href="lime.html#advantages-12"><i class="fa fa-check"></i><b>5.8.4</b> Advantages</a></li>
<li class="chapter" data-level="5.8.5" data-path="lime.html"><a href="lime.html#disadvantages-12"><i class="fa fa-check"></i><b>5.8.5</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.9</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.9.1" data-path="anchors.html"><a href="anchors.html#finding-anchors"><i class="fa fa-check"></i><b>5.9.1</b> Finding Anchors</a></li>
<li class="chapter" data-level="5.9.2" data-path="anchors.html"><a href="anchors.html#complexity-and-runtime"><i class="fa fa-check"></i><b>5.9.2</b> Complexity and Runtime</a></li>
<li class="chapter" data-level="5.9.3" data-path="anchors.html"><a href="anchors.html#tabular-data-example"><i class="fa fa-check"></i><b>5.9.3</b> Tabular Data Example</a></li>
<li class="chapter" data-level="5.9.4" data-path="anchors.html"><a href="anchors.html#advantages-13"><i class="fa fa-check"></i><b>5.9.4</b> Advantages</a></li>
<li class="chapter" data-level="5.9.5" data-path="anchors.html"><a href="anchors.html#disadvantages-13"><i class="fa fa-check"></i><b>5.9.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.9.6" data-path="anchors.html"><a href="anchors.html#software-and-alternatives-4"><i class="fa fa-check"></i><b>5.9.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.10</b> Shapley Values</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shapley.html"><a href="shapley.html#general-idea"><i class="fa fa-check"></i><b>5.10.1</b> General Idea</a></li>
<li class="chapter" data-level="5.10.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>5.10.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="5.10.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>5.10.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="5.10.4" data-path="shapley.html"><a href="shapley.html#advantages-14"><i class="fa fa-check"></i><b>5.10.4</b> Advantages</a></li>
<li class="chapter" data-level="5.10.5" data-path="shapley.html"><a href="shapley.html#disadvantages-14"><i class="fa fa-check"></i><b>5.10.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.10.6" data-path="shapley.html"><a href="shapley.html#software-and-alternatives-5"><i class="fa fa-check"></i><b>5.10.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.11</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.11.1" data-path="shap.html"><a href="shap.html#definition"><i class="fa fa-check"></i><b>5.11.1</b> Definition</a></li>
<li class="chapter" data-level="5.11.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.11.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.11.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.11.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.11.4" data-path="shap.html"><a href="shap.html#examples-4"><i class="fa fa-check"></i><b>5.11.4</b> Examples</a></li>
<li class="chapter" data-level="5.11.5" data-path="shap.html"><a href="shap.html#shap-feature-importance"><i class="fa fa-check"></i><b>5.11.5</b> SHAP Feature Importance</a></li>
<li class="chapter" data-level="5.11.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.11.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.11.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.11.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.11.8" data-path="shap.html"><a href="shap.html#shap-interaction-values"><i class="fa fa-check"></i><b>5.11.8</b> SHAP Interaction Values</a></li>
<li class="chapter" data-level="5.11.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.11.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.11.10" data-path="shap.html"><a href="shap.html#advantages-15"><i class="fa fa-check"></i><b>5.11.10</b> Advantages</a></li>
<li class="chapter" data-level="5.11.11" data-path="shap.html"><a href="shap.html#disadvantages-15"><i class="fa fa-check"></i><b>5.11.11</b> Disadvantages</a></li>
<li class="chapter" data-level="5.11.12" data-path="shap.html"><a href="shap.html#software-4"><i class="fa fa-check"></i><b>5.11.12</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> Example-Based Explanations</a><ul>
<li class="chapter" data-level="6.1" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>6.1</b> Counterfactual Explanations</a><ul>
<li class="chapter" data-level="6.1.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>6.1.1</b> Generating Counterfactual Explanations</a></li>
<li class="chapter" data-level="6.1.2" data-path="counterfactual.html"><a href="counterfactual.html#example-8"><i class="fa fa-check"></i><b>6.1.2</b> Example</a></li>
<li class="chapter" data-level="6.1.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-16"><i class="fa fa-check"></i><b>6.1.3</b> Advantages</a></li>
<li class="chapter" data-level="6.1.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-16"><i class="fa fa-check"></i><b>6.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.1.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>6.1.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> Adversarial Examples</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>6.2.1</b> Methods and Examples</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>6.2.2</b> The Cybersecurity Perspective</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> Prototypes and Criticisms</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#theory-5"><i class="fa fa-check"></i><b>6.3.1</b> Theory</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#examples-5"><i class="fa fa-check"></i><b>6.3.2</b> Examples</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#advantages-17"><i class="fa fa-check"></i><b>6.3.3</b> Advantages</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#disadvantages-17"><i class="fa fa-check"></i><b>6.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>6.3.5</b> Code and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> Influence Functions</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.3</b> Advantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.4</b> Disadvantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-6"><i class="fa fa-check"></i><b>6.4.5</b> Software and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Network Interpretation</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> Learned Features</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#feature-visualization"><i class="fa fa-check"></i><b>7.1.1</b> Feature Visualization</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#network-dissection"><i class="fa fa-check"></i><b>7.1.2</b> Network Dissection</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#advantages-18"><i class="fa fa-check"></i><b>7.1.3</b> Advantages</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#disadvantages-18"><i class="fa fa-check"></i><b>7.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#software-and-further-material"><i class="fa fa-check"></i><b>7.1.5</b> Software and Further Material</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html"><i class="fa fa-check"></i><b>7.2</b> Pixel Attribution (Saliency Maps)</a><ul>
<li class="chapter" data-level="7.2.1" data-path="pixel-attribution.html"><a href="pixel-attribution.html#vanilla-gradient-saliency-maps"><i class="fa fa-check"></i><b>7.2.1</b> Vanilla Gradient (Saliency Maps)</a></li>
<li class="chapter" data-level="7.2.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html#deconvnet"><i class="fa fa-check"></i><b>7.2.2</b> DeconvNet</a></li>
<li class="chapter" data-level="7.2.3" data-path="pixel-attribution.html"><a href="pixel-attribution.html#grad-cam"><i class="fa fa-check"></i><b>7.2.3</b> Grad-CAM</a></li>
<li class="chapter" data-level="7.2.4" data-path="pixel-attribution.html"><a href="pixel-attribution.html#guided-grad-cam"><i class="fa fa-check"></i><b>7.2.4</b> Guided Grad-CAM</a></li>
<li class="chapter" data-level="7.2.5" data-path="pixel-attribution.html"><a href="pixel-attribution.html#smoothgrad"><i class="fa fa-check"></i><b>7.2.5</b> SmoothGrad</a></li>
<li class="chapter" data-level="7.2.6" data-path="pixel-attribution.html"><a href="pixel-attribution.html#examples-6"><i class="fa fa-check"></i><b>7.2.6</b> Examples</a></li>
<li class="chapter" data-level="7.2.7" data-path="pixel-attribution.html"><a href="pixel-attribution.html#advantages-19"><i class="fa fa-check"></i><b>7.2.7</b> Advantages</a></li>
<li class="chapter" data-level="7.2.8" data-path="pixel-attribution.html"><a href="pixel-attribution.html#disadvantages-19"><i class="fa fa-check"></i><b>7.2.8</b> Disadvantages</a></li>
<li class="chapter" data-level="7.2.9" data-path="pixel-attribution.html"><a href="pixel-attribution.html#software-5"><i class="fa fa-check"></i><b>7.2.9</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html"><i class="fa fa-check"></i><b>7.3</b> Detecting Concepts</a><ul>
<li class="chapter" data-level="7.3.1" data-path="detecting-concepts.html"><a href="detecting-concepts.html#tcav-testing-with-concept-activation-vectors"><i class="fa fa-check"></i><b>7.3.1</b> TCAV: Testing with Concept Activation Vectors</a></li>
<li class="chapter" data-level="7.3.2" data-path="detecting-concepts.html"><a href="detecting-concepts.html#example-9"><i class="fa fa-check"></i><b>7.3.2</b> Example</a></li>
<li class="chapter" data-level="7.3.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html#advantages-20"><i class="fa fa-check"></i><b>7.3.3</b> Advantages</a></li>
<li class="chapter" data-level="7.3.4" data-path="detecting-concepts.html"><a href="detecting-concepts.html#disadvantages-20"><i class="fa fa-check"></i><b>7.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="7.3.5" data-path="detecting-concepts.html"><a href="detecting-concepts.html#bonus-other-concept-based-approaches"><i class="fa fa-check"></i><b>7.3.5</b> Bonus: Other Concept-based Approaches</a></li>
<li class="chapter" data-level="7.3.6" data-path="detecting-concepts.html"><a href="detecting-concepts.html#software-6"><i class="fa fa-check"></i><b>7.3.6</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="8.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>8.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="8.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>8.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> Contribute to the Book</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> Citing this Book</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> Translations</a></li>
<li class="chapter" data-level="12" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>12</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used.html"><a href="r-packages-used.html"><i class="fa fa-check"></i>R Packages Used</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decomposition" class="section level2">
<h2><span class="header-section-number">5.1</span> Functional Decompositon</h2>
<p>A prediction function is, or often can be simplified to, a function that takes a high-dimensional input and outputs a 1-dimensional number.
Functional decomposition takes this high-dimensional function and splits it into lower-dimensional components.
The split allows us to attribute effects to individual features and to identify interactions between features.
Functional decomposition is both an interpretation method and mental model which immensily helps you to understand other interpretation methods.</p>
<p>This chapter may be the most central in this book.
While a full functional decomposition is usually infeasible in practice, it is very valuable to understand the idea of functional decomposition.
Functional decomposition is a fundamental principal that underlies many other techniques, or at least can act as a mental lens through which to understand better what other methods do.</p>
<!-- Intuition -->
<p>We start with a function that takes two features as input and produces a 1-dimensional output, the prediction.
The function is:</p>
<p><span class="math display">\[y = f(x_1, x_2) = 2 + e^{x_1} - x_2 + x_1 \cdot x_2\]</span></p>
<p>We can visualize the function with a 3-D plot or a heatmap with contour lines:</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="images/unnamed-chunk-12-1.png" alt="Prediction surface of a two-dimensional function." width="1050" />
<p class="caption">
FIGURE 5.2: Prediction surface of a two-dimensional function.
</p>
</div>
<p>The function takes on large values when <span class="math inline">\(x_1\)</span> is large and <span class="math inline">\(x_2\)</span> is low, and it takes on low values for large <span class="math inline">\(x_2\)</span> and low <span class="math inline">\(x_1\)</span>.
We can see both from the formula and the figure that there is an interaction between the two features, meaning that the prediction is not merely a sum of each feature effect.
Note that in a machine learning setup nobody will give us such a neat formula, but we usually have access to the prediction/classification function.
Can we identify, just by having access to the function, the main effects and the interaction effect?
This would tell us how each feature affects the prediction individually, and which part is due to the interaction of both features.
The solution is functional decomposition: to decompose this two-dimensional function into its components.
For a two-dimensional function f, which only depends on two input features: <span class="math inline">\(f(x_1, x_2)\)</span>, we want each component to represent a main effect, interaction or intercept:</p>
<p><span class="math display">\[f(x_1, x_2) = f_0 + f_1(x_1) + f_2(x_2) + f_{1,2}(x_{1,2})\]</span></p>
<p>The component <span class="math inline">\(f_0\)</span> is the intercept, components <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> are the main effects of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and <span class="math inline">\(f_{1,2}\)</span> is the interaction effect between the two features.
The main effects tell us how each feature affects the prediction, independent of the values the other feature takes on.
The interaction effect tells us what the effect of the features is together.
The intercept simply tells us what the prediction is when all feature effects are set to zero.
Note that the components themselves are functions (except for the intercept) with different input dimensionalities.</p>
<p>Let us decompose the function above.
I will just give you the components for now, and the explanation for how to derive at the decomposition later.
The intercept is <span class="math inline">\(f_0\sim3.18\)</span>.
Since the other components are functions, we can visualize them:</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-13"></span>
<img src="images/unnamed-chunk-13-1.png" alt="Decomposition of a function." width="1050" />
<p class="caption">
FIGURE 5.3: Decomposition of a function.
</p>
</div>
<p>Do you think the components make sense given the true formula above?
For now it is a bit mysterious why the intercept has this specific value.
The feature <span class="math inline">\(x_1\)</span> shows an exponential effect as main effect, and <span class="math inline">\(x_2\)</span> shows a negative linear effect.
This matches the formula that produced the predictions.
The interaction term looks like a pringles.
In more mathematical terms, it is a hyperbolic paraboloid as we would expect for <span class="math inline">\(x_1 \cdot x_2\)</span>.</p>
<p>The example was two-dimensional.
Before we go into the details how the components were computed, we define the decomposition for a more general setup.</p>
<div id="deeper-into-the-mathematics" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Deeper Into the Mathematics</h3>
<p>We start with a function that takes <span class="math inline">\(p\)</span> features as input, <span class="math inline">\(f: \mathbb{R}^p \mapsto \mathbb{R}\)</span>.
This could be a regression function, but also the classification probability for a certain class or the score for a certain cluster (unsupervised machine learning).
How does a composition look in general?</p>
<p><span class="math display">\[\begin{align*}f(x) = &amp; f_0 + f_1(x_1) + \ldots + f_p(x_p) \\ &amp; + f_{1,2}(x_1, x_2) + \ldots + f_{1,p}(x_1, x_p) + \ldots + f_{p-1,p} \\  &amp; + \ldots  \\ &amp; +  f_{1,\ldots,p}(x_1, \ldots, x_p)\end{align*}\]</span></p>
<p>We can make the decomposition look a bit mathematically nicer by indexing all possible subsets of feature combinations: <span class="math inline">\(S\subseteq\{1,\ldots,p\}\)</span>.
This set contains the intercept (<span class="math inline">\(S=\emptyset\)</span>), main effects (<span class="math inline">\(|S|=1\)</span>), and all interactions (<span class="math inline">\(|S|\leq{}1\)</span>).
With this subset defined, we can write the decomposition as:</p>
<p><span class="math display">\[f(x) = \sum_{S\subseteq\{1,\ldots,p\}} f_S(x_S)\]</span></p>
<p>Here, <span class="math inline">\(x_S\)</span> is the vector of features in the index set <span class="math inline">\(S\)</span>.
Just how many components are those?
A function that takes in a p-dimensional vector can be split into <span class="math inline">\(\sum_{i=0}^p\binom{p}{i}=2^p\)</span> components.
For example, if the function has 10 features, we we can split it into 1042 components: 1 intercept, 10 main effects, 90 2-way interaction terms, 720 3-way interaction terms, …
With each additional feature, the number of components doubles.
Clearly it is not feasible to compute all the components for most functions.
Another reasons to NOT compute all components is that components with <span class="math inline">\(|S|&gt;2\)</span> are difficult to visualize and interpret.</p>
</div>
<div id="how-to-compute-the-components" class="section level3">
<h3><span class="header-section-number">5.1.2</span> How to Compute the Components</h3>
<p>So far I have avoided the topic of how the components are defined and computed.
Without any further constraints on how these components should behave, there is actually no unique solution.
So far, the only restrictions we talked about were that the sum of components should yield the function again, and about the dimensionalities of each component.
Because without further constraints, we are free to move effects between main effects and feature effects.
For example we could say in the example above that one of the main effects is zero, and add its effects to the interaction.
In this case the sum of all components is still the complete function.</p>
<p>Here is an even more extreme example which highlights the need for restrictions for the components:
Let’s say you have a 3-dimensional function.
It actually does not matter how this function looks, but the following decomposition would <strong>always</strong> be valid:
<span class="math inline">\(f_0\)</span> is 0.12.
<span class="math inline">\(f_1 = 2 \cdot x_1\)</span> + number of shoes you own.
<span class="math inline">\(f_2\)</span>, <span class="math inline">\(f_3\)</span>, <span class="math inline">\(f_{1,2}\)</span>, <span class="math inline">\(f_{2,3}, f_{1,3}\)</span> are all zero.
And finally to make this trick work, I define:</p>
<p><span class="math display">\[f_{1,2,3} = f(x) - \sum_{S\subset\{1,\ldots,p\}} f_S(x_S)\]</span></p>
<p>Not very meaningful, and quite deceptive if you would present this as the interpretation of your model.
How do we prevent this ambiguity?
Here is where different methods make different assumptions.</p>
<p>In this chapter, we will discuss three methods:</p>
<ul>
<li>Statistical regression models</li>
<li>Functional ANOVA</li>
<li>Generalized Functional ANOVA</li>
<li><a href="ale.html#ale">Accumulated Local Effects</a> is discussed in another chapter</li>
</ul>
</div>
<div id="statistical-regression-models" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Statistical Regression Models</h3>
<p>This first approach links back to <a href="simple.html#simple">interpretable models</a>, especially <a href="extend-lm.html#extend-lm">generalized additive models</a>.
Instead of decomposing a complex function, we can build restrictions into the modeling process so that we can simply read out the individual components.
While decomposition is a top-down approach, where you start with high-dimensional function and break it down, these regression models provide a more bottom-up approach, where you build up your model from simple components.
Both approaches have in common that we aim to have individual components that we may interpret.
For statistical models, we restrict the number of components, so that our model does not have to fit all the <span class="math inline">\(2^p\)</span> components.</p>
<p>Let us go even simpler and consider the linear regression function:</p>
<p><span class="math display">\[f(x) = \beta_0 + \beta_1 x_1 + \ldots \beta_p x_p\]</span></p>
<p>This looks awfully similar to the functional decomposition, but with two major modifications.
Modification 1: All interaction effects are excluded and we only keep intercept and main effects.
Modification 2: The main effects are only allowed to be linear in the feature: <span class="math inline">\(f_j(x_j)=\beta_j{}x_j\)</span>.
Viewing the linear regression model through the lens of functional decomposition, we can see that the model itself represents a functional decomposition of the true function that maps from features to target, but with strong assumptions of no interactions and linear effects.</p>
<p>The generalized additive model relaxes the second assumption, as it allows more flexible function <span class="math inline">\(f_j\)</span> by using splines.
Interactions can also be added, but this process is rather manual.
Extensions to the GAM try to add 2-way interactions automatically. CITE GA2M.</p>
<p>There is also some potential for confusion:
When you have a linear regression model and add an interaction term, this decomposition will be different from when you apply one of the following functional decomposition approaches to the linear regression model.
An additional requirement for this mismatch to occur is that the interacting features need to be correlated.
The mismatch occurs because other functional decomposition approaches split the effects differently between interactions and main effect.</p>
<p>When should we use statistical models instead of more complex models that we decompose post-hoc?
If most interactions are zero, especially the ones with 3 or more features.
If we know that the maximum number of features involved in interactions is 2, i.e., <span class="math inline">\(|S|\leq{}2\)</span>, then we can use approaches like MARS or GA2M.</p>
</div>
<div id="functional-anova" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Functional ANOVA</h3>
<p>The functional ANOVA was proposed by Hooker (2004)<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a>.</p>
<p>The requirement is that the model prediction function f is square integrable.
As with any functional decomposition, the functional ANOVA decomposes the function into components:</p>
<p><span class="math display">\[f(x) = \sum_{S\subseteq\{1,\ldots,p\}} f_S(x_S)\]</span></p>
<p>Hooker (2004) proposes to estimate each components as:</p>
<p><span class="math display">\[f_S(x) = \int_{X_{-S}} \left( f(x) - \sum_{V \subset S} f_V(x)\right) d X_{-S})\]</span></p>
<p>Ok, let us take this thing apart.
We can rewrite the component as:</p>
<p><span class="math display">\[f_S(x) = \int_{X_{-S}} \left( f(x)\right) d X_{-S}) - \int_{X_{-S}} \left(\sum_{V \subset S} \right) d X_{-S})\]</span></p>
<p>The first part is the integral over the prediction function with respect to the features that are not in the set of components <span class="math inline">\(S\)</span>, which is referred to with the <span class="math inline">\(-S\)</span>.
For example if we compute the 2-way component for features 2 and 3, we would integrate over feature 1, 4, …
The integral can also be seen as the expected value of the prediction function with respect to <span class="math inline">\(X_{-S}\)</span>, and pretending that all features follow a uniform distribution.
We subtract from this interval all the lower dimensional components.
This subtraction has the effect of kind of centering and also removing the effect of all lower-order effects.
For example if <span class="math inline">\(S=\{2,3\}\)</span> we would subtract the main effects of both features <span class="math inline">\(f_2\)</span> and <span class="math inline">\(f_3\)</span>, and the intercept <span class="math inline">\(f_0\)</span>.
The occurence of all lower-order effects also means that the formula is recursive: we have to go all the way to the intercept and compute all effects from intercept to one level below the effect we are interested in.
For the intercept <span class="math inline">\(f_0\)</span>, the subset is the empty set <span class="math inline">\(S=\{\emptyset\}\)</span> and therefore -S contains all features <span class="math inline">\(XS\)</span>.
Therefore we get:</p>
<p><span class="math display">\[f_S(x) = \int_{X} f(x) dX\]</span></p>
<p>This is simply the prediction function where we integrated over all features.
And it can also be seen as the expectation of the prediction function when we assume that all features are uniformly distributed.
Now that we have <span class="math inline">\(f_0\)</span>, we can get the formula for <span class="math inline">\(f_1\)</span> (and equivalently for <span class="math inline">\(f_2\)</span>):</p>
<p><span class="math display">\[f_1(x) = \int_{X_{-1}} \left( f(x) - f_0\right) d X_{-S}\]</span></p>
<p>To finish the calculation for component <span class="math inline">\(f_{1,2}\)</span> we can put everything together:</p>
<p><span class="math display">\[\begin{align*}f_{1,2}(x) &amp;= \int{\{3,4\}} \left( f(x) - (f_0(x) + f_1(x) - f_0 + f_2(x) - f_0)\right) d X_{3},X_4 \\  &amp;= \int{\{3,4\}} \left(f(x) - f_1(x) - f_2(x) + f_0)\right) d X_{3},X_4 \end{align*}\]</span></p>
<p>This example shows how each higher order effect is defined by integrating over all other features, but also by removing all the lower-order effects that are subsets of the higher-order effects.</p>
<p>Hooker (2004) showed that this definition of functional components fullfills these (desirable) axioms:</p>
<ul>
<li>Zero Means: <span class="math inline">\(\int{}f_S(x_S)dX_s=0\)</span> for each <span class="math inline">\(S\neq\emptyset\)</span>.</li>
<li>Orthogonality: <span class="math inline">\(\int{}f_S(x_S)f_V(x_v)dX=0\)</span> for <span class="math inline">\(S\neq{}V\)</span></li>
<li>Variance Decomposition: Let <span class="math inline">\(\sigma^2_{f}=\int{}f(x)^2dX\)</span>, then
<span class="math display">\[ \sigma^2(f) = \sum_{S \subseteq P} \sigma^2_S(f_S),\]</span>
where P is the set of all features.</li>
</ul>
<p>The zero means axiom implies that all effects or interactions are centered around zero.
As a consequence, the interpretation at a position x is relative to the mean, and not the absolute prediction.</p>
<p>The orthogonality axiom says that any two components do not share information, meaning that, for example, the first order effect of feature <span class="math inline">\(X_1\)</span> and the interaction term of <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_2\)</span> are not correlated.
Because of orthogonality, all components are “pure” in the sense that they don’t mix effects.
It makes a lot of sense that the component for, say, feature <span class="math inline">\(X_4\)</span> should be independent of the interaction term between features <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.
The more interesting consequence is for orthogonality of hierarchical components, where one component contains features of another, for example, interaction between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, and the main effect of feature <span class="math inline">\(X_1\)</span>.
A two-dimensional partial dependence plot for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> would contain four effects: the intercept, the two main effects of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> and the interaction between them.
The funtional ANOVA component for <span class="math inline">\((X_1,X_2)\)</span> only contains the interaction, and with the main effects.</p>
<p>The variance decomposition allows us to split the variance of the function <span class="math inline">\(f\)</span> among the components, and guarantees that it really adds up in the end.
The variance decomposition property can also explain to us why the method is called ``functional ANOVA’’.
In statistics ANOVA stands for ANalysis Of VAriance.
ANOVA refers to a collection of methods that analyze differences of the mean of a target variable.
ANOVA works by dividing up the variance and attributing it to other variables.
Functional ANOVA is an extension of that concept to arbitrary functions.</p>
<!--
CONTINUE HERE

some notes on generalized fANOVA chapter:
- make it short
- say its complicated
- hint to porblems

-->
<p>```</p>
</div>
<div id="generalized-functional-anova-for-dependent-features" class="section level3">
<h3><span class="header-section-number">5.1.5</span> Generalized functional ANOVA for dependent features</h3>
<p>Similar to most interpretation techniques based on sampling data (like PDP), the functional ANOVA can produce misleading results when features are correlated.
If we simply integrate over the marginal distribution, we effectively create a new dataset which does not match with the joint distribution, but extrapolates to areas outside of the joint distribution.
Is it possible to define a decomposition which fulfills the desired axioms, but in addition does not create data outside the distribution?
The answer is yes, as long as we know (or can approximate) the joint distribution.</p>
<p>Hooker (2007) <a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a> proposed the generalized functional ANOVA, a decomposition that works for dependent features.
It is a generalization of the functional ANOVA that we encountered before, meaning that the functional ANOVA is just a special case of the generalized functional ANOVA.
The components are defined in terms of projections of f onto the space of additive functions:</p>
<p><span class="math display">\[f_S(x_S) | S \subset P = argmin_{g_S \in L^2(\mathbb{R}^S)_{S \in P}} \int \left(\sum_{\S \subset P} g_S(x_S) - f(x)\right)^2 w(x)dx.\]</span></p>
<p>Instead of orthogonality, the components are defined for the hierarchical orthogonality condition:</p>
<p><span class="math display">\[\forall f_S(x_S)| S \subset U: \int f_S(x_S) f_U(x_U) w(x)dx = 0\]</span></p>
<p><span class="math display">\[\forall f_S(x_S)| S \subset U: \int f_S(x_S) f_U(x_U) w(x)dx_i dx_{-u} = 0\]</span></p>
<p>Note that this only applies to hierachical components and if <span class="math inline">\(S \notsubset U\)</span> then the two components <span class="math inline">\(f_S\)</span> and <span class="math inline">\(f_U\)</span> do not have to be orthogonal.
That’s important, because that can lead to mixing direct and indirect effects, similar to the M-Plot in the <a href="ale.html#ale">ALE</a> chapter.</p>
<p>Without this constraint, the solution would not be unique.
We have not defined yet what <span class="math inline">\(w(x)\)</span> really is.
One solution would be to let <span class="math inline">\(w\)</span> be the uniform measure on the unit cube.
The natural choice would of course be that w is the joint probability function.</p>
<p>The estimation is done on a grid of points.
We have two problems when doing the estimation.</p>
<p>First problem is that we need to define <span class="math inline">\(w(x)\)</span>.
While the joint distribution is a good choice, there are few cases where we actually know the joint distribution.
The second problem is that we solve this in linear regression and have to do this theoretically for all components at once, meaning to solve this in a model with <span class="math inline">\(2^{P - 1}\)</span> components.
For this for each feature a grid of feature values is defined.</p>
<p>Notes on the paper:</p>
<ul>
<li>fANOVA: based on integration operators. gfANOVA: based on projections of function onto spaces of additive functions</li>
<li>function f and weight function w are assumed to be known -&gt; for f usually the case, but not for w</li>
<li>Results may be highly variable</li>
<li>estimation based on grid points</li>
<li>Projection: $&lt;f,g&gt;<em>{w} = </em>{^d} f(x)g(x)w(x)dx $</li>
<li>when w(x) is the uniform measure on the unit cube, the decomposition results in the special case of the functional ANOVA</li>
<li>I believe it should be possible to define this also on the marginal distribution to get close to the PDP</li>
<li>Natural choice for w is the joint probability</li>
<li>But also other choice for w can be meaningful: for example a local measure to get local decomp. or just exclude regions</li>
<li>example from paper
<ul>
<li>4dim function with <span class="math inline">\(X_1,X_2,X_3,X_4\)</span>, and interested in interaction between features one and two: <span class="math inline">\(f_{12}\)</span></li>
<li><span class="math inline">\(f_{12}\)</span> must be orthogonal to <span class="math inline">\(f_{123},f_{124},f_{1234}\)</span>.</li>
</ul></li>
</ul>
<p>Question:</p>
<ul>
<li>Can I estimate components individually?</li>
</ul>
<p>Notes from ALE paper:</p>
<ul>
<li>ALE decomposition follows a pseudo-orthogonality. It kind of says that if we apply ALE to another ALE components (instead of the prediction function, as you would normally do), them the resulting ALE effect is zero. In other words: the ALE operator (which maps a function to an ALE component) maps onto orthogonal subspaces of an inner product space.
<ul>
<li>Example: We compute <span class="math inline">\(f_{ALE,12}\)</span> for feature 1 and 2. Then we compute ALE for features 2 and 3, but on <span class="math inline">\(f_{ALE,12}\)</span> then answer is 0</li>
<li>It also means that the in second-order effects if we look for first-order effects, they are zero</li>
<li>simple example: <span class="math inline">\(f(x) = x_1 + x_2\)</span> with correlation between the two features. aLE will be the <span class="math inline">\(f_1 = x_1\)</span> and <span class="math inline">\(f_2 = x_2\)</span>, but generalized functional anova would mix both, e.g. <span class="math inline">\(f_1\)</span> would be a function of both <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span></li>
</ul></li>
<li>Advantages of ALE decomp over gfANOVA:
<ul>
<li>pseudo-orthogonality more desirable than hierarchical orthogonality</li>
<li>gfANOVA requires p(x), ALE does not</li>
<li>gfANOVA requires simultaneous computation of components, requiring go solve a complex system of equations. ALE components can be estimated hierarchically</li>
</ul></li>
</ul>
</div>
<div id="viewing-other-methods-through-the-lens-of-decomposition" class="section level3">
<h3><span class="header-section-number">5.1.6</span> Viewing Other Methods Through the Lens of Decomposition</h3>
<p>You might want to come back to this chapter again if you have a good grasp on some of the other methods.</p>
<p>First on a high level:
Feature effects are direct visualizations of the individual components.
However we have to distinguish between total effect and isolated effect for the higher-order feature effects.
PDP is total effect
ALE is individual effect
If you remove lower effects from PDP, you get fANOVA, at least for indepdentn feature case.</p>
<p>PDP is a direct decomposition, but with additional intercept difference.
ALE is a decomposition.
For permutation feature importance,
Methods such as SHApley and co only describe a prediction with 1-dimensional effects.
What happened with the interaction terms? They are divided among the individual effects.
What happened in PFI with the interactions? They are alos divided among individual effects.</p>
<p>The SHAP interaction plots can also be better understood through decompositions.</p>
<p>There are many methods that produce individual explanations in the form of <span class="math inline">\(f(X)=\sum_{p=1}^n\phi_j\)</span>, which attribute one number per feature, see <a href="shapley.html#shapley">Shapley Values</a>, <a href="lime.html#lime">LIME</a> and most <a href="pixel-attribution.html#pixel-attribution">pixel attribution methods for neural networks</a>, all of which you will encounter later in the book.
Looking at them through functional decomposition:
When methods fulfill the property of efficiency, like for example Shapley values do, it means that the sum of attributions are equal to the prediction.
This means that we decomposed the function.
But there are only first order effects, no interactions.
It means that all the interactions have to be split among the individual values per feature.
And we do not get separate interation effects.</p>
<p>Can a function exist where lower-components are zero, but the the interactions are non-zero?
For example, components for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are zero, but their interaction is not?
Yes!
For example, the XOR problem where <span class="math inline">\(Y=X_1XOR{}X_2\)</span>.</p>
</div>
<div id="advantages-5" class="section level3">
<h3><span class="header-section-number">5.1.7</span> Advantages</h3>
<p>A rigorous way of thinking about high-dimensional functions.</p>
<p>Allows to understand most other interpretation methods much better.</p>
<p>Separation and attribution of effects.</p>
</div>
<div id="disadvantages-5" class="section level3">
<h3><span class="header-section-number">5.1.8</span> Disadvantages</h3>
<p>When output high-dimensional, than not as simple.</p>
<p>Makes little sense for images and text.</p>
<p>No clear superior way for the axioms.</p>
<p>Estimating higher effect components always difficult and no good way to visualize.</p>
<p>Functional ANOVA is very difficult to estimate, no available software.</p>
<p>ALE is an alternative for when features are dependent, but offers no variance decomposition.</p>
<p>Does not have an implementation besides (ALE)[#ale].</p>

<!--{pagebreak}-->
</div>
</div>
<div class="footnotes">
<hr />
<ol start="27">
<li id="fn27"><p>Hooker, Giles. “Discovering additive structure in black box functions.” Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. 2004.<a href="decomposition.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p>Hooker, Giles. “Generalized functional anova diagnostics for high-dimensional functions of dependent variables.” Journal of Computational and Graphical Statistics 16.3 (2007): 709-732.<a href="decomposition.html#fnref28" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="agnostic.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pdp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/manuscript/05.1-agnostic-decomposition.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
